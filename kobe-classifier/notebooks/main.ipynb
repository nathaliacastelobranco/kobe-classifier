{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final - Engenharia de Machine Learning\n",
    "\n",
    "**Gerar um .md e mover para a pasta docs com as respostas**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A solução criada nesse projeto deve ser disponibilizada em repositório git e disponibilizada em servidor de repositórios (Github (recomendado), Bitbucket ou Gitlab). O projeto deve obedecer o Framework TDSP da Microsoft. Todos os artefatos produzidos deverão conter informações referentes a esse projeto (não serão aceitos documentos vazios ou fora de contexto). Escreva o link para seu repositório. \n",
    "\n",
    "INSERIR ARTEFATOS NA PASTA DOCS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Iremos desenvolver um preditor de arremessos usando duas abordagens (regressão e classificação) para prever se o \"Black Mamba\" (apelido de Kobe) acertou ou errou a cesta.\n",
    "Para começar o desenvolvimento, desenhe um diagrama que demonstra todas as etapas necessárias em um projeto de inteligência artificial desde a aquisição de dados, passando pela criação dos modelos, indo até a operação do modelo.'\n",
    "\n",
    "INSERIR DIAGRAMA NA PASTA DOCS >> PROJECT CHARTER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Descreva a importância de implementar pipelines de desenvolvimento e produção numa solução de aprendizado de máquinas.\n",
    "\n",
    "    Os pipelines são encadeamentos de processos, como uma esteira de dados, onde existe um mapeamento das funções e processos relacionados à uma solução de aprendizagem de máquina. A ideia de utilizar pipelines no desenvolvimento de soluções de ML é diretamente relacionado à reprodutibilidade e à ideia de modularização e microserviços. \n",
    "\n",
    "    Exemplificando sua importância:\n",
    "        Um projeto de ML com uma entrada de dados e diversos modelos testados para eleger o modelo vencedor, pode ser dividido em alguns pipelines e caso exista a necessidade de manutenção em um dos serviços, seja o de entrada de dados, seja um modelo existente ou a criação de um novo modelo, encadear todo o processo em pipelines torna o projeto mais fácil de gerir e fazer manutenções."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Como as ferramentas Streamlit, MLFlow, PyCaret e Scikit-Learn auxiliam na construção dos pipelines descritos anteriormente? A resposta deve abranger os seguintes aspectos:\n",
    "\n",
    "a. Rastreamento de experimentos;\n",
    "\n",
    "b. Funções de treinamento;\n",
    "\n",
    "c. Monitoramento da saúde do modelo;\n",
    "\n",
    "d. Atualização de modelo;\n",
    "\n",
    "e. Provisionamento (Deployment)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Com base no diagrama realizado na questão 2, aponte os artefatos que serão criados ao longo de um projeto. Para cada artefato, indique qual seu objetivo.\n",
    "\n",
    "    No TDSP da microsoft, os artefatos são os documentos escritos (em verde no gráfico). Os obrigatórios são o Project Charter, que é a visão do todo (o diagrama da questão 2) e o Project Final Report, que é a dashboard gerada no Streamlit.\n",
    "\n",
    "    * Project Charter - Criado inicialmente para gerir e direcionar a construção do projeto de machine learning, foi desenvolvido um diagrama conforme a questão 2\n",
    "    * Project Final Report - Criado para visualizar e monitorar a implantação planejada no Project Charter\n",
    "\n",
    "    Para os artefatos dos pipelines e no MLflow, o que é considerado como artefatos são os arquivos de dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Implemente o pipeline de processamento de dados com o mlflow, rodada (run) com o nome \"PreparacaoDados\":\n",
    "\n",
    "    * a. Os dados devem estar localizados em \"/Data/kobe_dataset.csv\" **OK - coloquei em data/01_raw**\n",
    "\n",
    "    * b. Observe que há dados faltantes na base de dados! As linhas que possuem dados faltantes devem ser desconsideradas. Você também irá filtrar os dados onde o valor de shot_type for igual à 2PT Field Goal. Ainda, para esse exercício serão apenas consideradas as colunas: \n",
    "\n",
    "        - i. lat\n",
    "        - ii. lng\n",
    "        - iii. minutes remaining\n",
    "        - iv. period\n",
    "        - v. playoffs\n",
    "        - vi. shot_distance\n",
    "\n",
    "    A variável *shot_made_flag* será seu alvo, onde 0 indica que Kobe errou e 1 que a cesta foi realizada. O dataset resultante será armazenado na pasta \"/Data/processed/data_filtered.parquet\". \n",
    "\n",
    "    Ainda sobre essa seleção, qual a dimensão resultante do dataset?\n",
    "        **(20285, 7)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Separe os dados em treino (80%) e teste (20 %) usando uma escolha aleatória e estratificada. \n",
    "\n",
    "- [ ] Armazene os datasets resultantes em \"/Data/processed/base_{train|test}.parquet . \n",
    "- [ ] Explique como a escolha de treino e teste afetam o resultado do modelo final. \n",
    "- [ ] Quais estratégias ajudam a minimizar os efeitos de viés de dados.\n",
    "\n",
    "**Stratified** https://scikit-learn.org/stable/modules/cross_validation.html#stratification\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\n",
    "\n",
    "**Perguntar ao meletti como ele fez a separação estratificada dos dados**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Registre os parâmetros (% teste) e métricas (tamanho de cada base) no MlFlow\n",
    "\n",
    "    Registrar como parâmetro em conf/base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "7. Implementar o pipeline de treinamento do modelo com o Mlflow usando o nome \"Treinamento\"\n",
    "\n",
    "a. Com os dados separados para treinamento, treine um modelo com regressão logística do sklearn usando a biblioteca pyCaret.\n",
    "\n",
    "b. Registre a função custo \"log loss\" usando a base de teste\n",
    "\n",
    "c. Com os dados separados para treinamento, treine um modelo de classificação do sklearn usando a biblioteca pyCaret. A escolha do algoritmo de classificação é livre. Justifique sua escolha.\n",
    "\n",
    "d. Registre a função custo \"log loss\" e F1_score para esse novo modelo\n",
    "\n",
    "8. Registre o modelo de classificação e o disponibilize através do MLFlow através de API. Selecione agora os dados da base de dados original onde shot_type for igual à 3PT Field Goal (será uma nova base de dados) e através da biblioteca requests, aplique o modelo treinado. Publique uma tabela com os resultados obtidos e indique o novo log loss e f1_score.\n",
    "\n",
    "a. O modelo é aderente a essa nova base? Justifique.\n",
    "\n",
    "b. Descreva como podemos monitorar a saúde do modelo no cenário com e sem a disponibilidade da variável resposta para o modelo em operação\n",
    "\n",
    " c. Descreva as estratégias reativa e preditiva de retreinamento para o modelo em operação.\n",
    "\n",
    "9. Implemente um dashboard de monitoramento da operação usando Streamlit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do List\n",
    "\n",
    "* [ ] Criar pasta com a base de dados no formato do PF\n",
    "* [ ] Obter os dados do Kaggle\n",
    "* [ ] Criar um pipeline para etapa de preparação dos dados chamado \"PreparacaoDados\"\n",
    "* [ ] Separar os dados em uma amostra estratificada, entender qual a melhor maneira de fazer isso\n",
    "* [ ] Demonstrar como ficou a separação dos dados, graficamente se possível, devido ao uso do stratified k-folds (6d)\n",
    "* [ ] Inserir o % de treino, teste, k-folds, entre outros como parâmetros no ML flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
